% !TEX program = xelatex
\documentclass[compress]{beamer}

\usepackage[english]{babel}
\usepackage{metalogo}
\usepackage{listings}
\usepackage{fontspec}
\usepackage{amsmath, amssymb}
\usepackage{stackrel}
\usepackage{tikz}
\usepackage{svg}
\usepackage{unicode-math}
\usepackage[theme=nord,charsperline=60,linenumbers]{jlcode}

\usetheme{Nord}

\setmainfont{Roboto}
% \setsansfont{DejaVu Serif}
\setmonofont{JuliaMono}

\newcommand{\E}[1]{\ensuremath{E\left\{#1\right\}}}

\AtBeginSection[]
{
    \begin{frame}[c,noframenumbering,plain]
        \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/show/hide]
    \end{frame}
}


\AtBeginSubsection[]
{
    \begin{frame}[c,noframenumbering,plain]
        \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
    \end{frame}
}


\title{Project I: Random Processes}
\subtitle{Random variables, stationarity \& ergodicity}
\author{Simon Andreas Bjørn}
\date{February 7, 2023}

\begin{document}

\begin{frame}[plain,noframenumbering]
    \maketitle
\end{frame}

\begin{frame}[fragile]
    \frametitle{1. Random variables}
    Importing some statistic and plotting modules
    \begin{jllisting}[gobble=8]
        using Printf
        using Statistics
        using NaNStatistics
        using Plots
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.1 Random variables - uniform PDF}
    We start by sampling the uniform destribution on $\left[0,1\right)$,
    with $N=10000$ samples.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = rand(N)
    \end{jllisting}
    \pause
    We then find the first order momentum $m_x = E\{x\}$ and
    the second order central moment $\sigma_x^2 = E\{(x-m_x)^2\}$
    \begin{jllisting}[gobble=8]
        # Evaluate E{x} and E{(x-mₓ)²} by definition
        mₓ  = sum(x*1/N)
        σₓ² = sum((x.-mₓ).^2 * 1/N)

        # Evaluate E{x} and E{(x-mₓ)²} by builtin functions
        mₓ_builtin = mean(x)
        σₓ²_builtin = std(x)^2
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculation differences}
    Calculating the difference between the builtin functions and our direct evaluation, we 
    see that the the two methods are similar.
    \begin{jllisting}[gobble=8]
        @printf "mₓ_err: %.4f" abs(mₓ-mₓ_builtin)
        # -> mₓ_err: 0.0000
        @printf "σₓ²_err: %.4f" abs(σₓ²-σₓ²_builtin)
        # -> σₓ²_err: 0.0000
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Calculating the PDF}
    We then calculate the pdf using \jlinl{histcounts}, and normalize the PDF.
    \begin{jllisting}[gobble=8]
        step_size = 0.05
        edges = -0.2:step_size:1.2
        bins = histcounts(x, edges)
        
        # Normalize bins to get PDF
        pdf = bins/sum(bins*step_size)

        # Theoretical PDF
        theoretical_pdf = (x) -> Float64(0 <= x < 1)
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Code for plotting the PDF}
    And we plot the result
    \begin{jllisting}[gobble=8]
        plot(
            edges[1:end-1],
            pdf,
            seriestype=:steppost, 
            label="Estimated PDF"
        )
            plot!(
            edges[1:end-1],
            theoretical_pdf.(edges[1:end-1]),
            seriestype=:steppost, label="Theoretical PDF"
        )
            plot!(
            legend=:bottom,
            background_color=:transparent,
            foreground_color=:white
        )
        title!("PDF of x");xlabel!("Value");
        ylabel!("Probability");savefig("uniform_pdf.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Plot of the PDF}
    \begin{figure}
        \includesvg[width=\columnwidth]{"../uniform_pdf.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Ensuring properties}
    We want to verify if our PDF fulfill the 2 following properties: 
    \begin{enumerate}
        \item $\forall \alpha, \ f_x(\alpha) \ge 0$ \\
            \jlinl{reduce(&, pdf .>= 0) # -> true}
        \item $\int_{-\infty}^{\infty} f_x(\alpha) \; \texttt{d}\alpha = 1$ \\ 
            \jlinl{abs(sum(pdf*step_size)-1.0) < 1e-12 # -> true}
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Deriving the theoretical mean [$m_x$]}
    \pause
    We start at the definition of the mean value which is given by
    \begin{equation}
        m_x = E\{x\} = \int_{-\infty}^{\infty}\alpha f_x(\alpha)\texttt{d}\alpha
    \end{equation}
    \pause
    We then use the theoretical probability density function of a uniform distribution
    which is given by 
    \begin{equation*}
        f_x(\alpha) = 
        \begin{cases}
            0, & \alpha < a \\
            \frac{1}{b-a}, & a \le \alpha \le b \\
            0, & \alpha > b 
        \end{cases}
    \end{equation*}
\end{frame}

\begin{frame}
    \frametitle{Deriving the theoretical mean [$m_x$]}
    With $a = 0, b = 1$, we get the theoretical PDF
    \begin{equation*}
        f_x(\alpha) = 
        \begin{cases}
            0, & \alpha < 0 \\
            1, & 0 \le \alpha \le 1 \\
            0, & \alpha > 1 
        \end{cases}
    \end{equation*}
    \pause
    We simplify eq.1, and get the integral
    \begin{equation*}
        m_x = E\{x\} = \int_0^1\alpha \ \texttt{d}\alpha
    \end{equation*}
    \pause
    We simply solve this and get the mean value to be
    \begin{equation*}
        m_x = \frac{1}{2} \left[\alpha^2\right]_0^1=\frac{1}{2}
    \end{equation*}
\end{frame}
\begin{frame}
    \frametitle{Deriving the theoretical variance [$\sigma_x^2$]}
    \pause
    Again, we start at the definition
    \begin{equation}
        \sigma_x^2=E\left\{(x-m_x)^2\right\}=\int_{-\infty}^{\infty}
        (\alpha - m_x) f_x( \alpha ) \texttt{d}\alpha
    \end{equation}
    \pause
    We do the same simplification as with the mean value, and and fill in for $m_x$.
    This gives the variance
    \begin{align*}
        \sigma_x^2=& \int_0^1\left(\alpha-\frac{1}{2}\right)^2\ \texttt{d}\alpha \\
        =& \int_0^1 \alpha^2-\alpha+\frac{1}{4}\ \texttt{d} \alpha \\
        =& \left[\frac{1}{3}\alpha^3-\frac{1}{2}\alpha^2+\frac{1}{4}\alpha\right]^1_0 \\
        =& \frac{1}{12} \approx 0.08334
    \end{align*}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Repeating the uniform experiment}
    We repeat the experiment, find the mean and variance of each experiment and
    accumulate the results.
    \begin{jllisting}[gobble=8]
        r = 50 # Repeat r times
        xs = rand(Float64, (N,r))
        ms = mean(xs, dims=1)
        σs = std(xs, dims=1).^2
        # Accumulate the results
        i_mean = [mean(ms[1:i]) for i in 1:length(ms)]
        i_std = [mean(σs[1:i])^2 for i in 1:length(s)]
        # Plot the results
        plot(1:r, i_mean, label="Cumulative mean", 
            legend=:bottomleft, xlabel="Experiment iteration",
            ylabel="Mean value")
        plot!( twinx(), i_std, label="Variance",
            legend=:bottomright, color=:orange, ylabel="Variance")
        plot!(foreground_color=:white, 
            background_color=:transparent)
        savefig!("repeated_experiment_uniform.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Repeating the uniform experiment}
    We can see that the mean and variance approach some values as we perform
    more experiments
    \begin{figure}
        \includesvg[width=0.8\columnwidth]{"../repeated_experiment_uniform.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.2 Gaussian PDF}
    We perform the same experiment, but sample using \jlinl{randn}.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = randn(N)
        mₓ  = sum(x*1/N)
        σₓ² = sum((x.-mₓ).^2 * 1/N)

        step_size = 0.1
        edges = -5.0:step_size:5.0
        bins = histcounts(x, edges)
        # Normalize PDF
        pdf = bins/sum(bins*step_size)
        theoretical_pdf = (x) -> 1.0/√(2π)*exp(-x.^2 ./ 2)
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.2 Plotting code}
    We then plot the results in a similar manner as 1.1.
    \begin{jllisting}[gobble=8]
        plot( edges[1:end-1], pdf, seriestype=:steppost,
            label="Estimated PDF", linewidth=2)

        plot!(edges[1:end-1], theoretical_pdf.(edges[1:end-1]),
            label="Theoretical PDF", linewidth=2)

        plot!(legend=:bottom, background_color=:transparent,
            foreground_color=:white)

        title!("PDF of x");xlabel!("Value");
        ylabel!("Probability");
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{1.2 Gaussian PDF plot}    
    \begin{figure}
        \includesvg[width=0.9\columnwidth]{../gaussian_pdf.svg}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{1.3 Central Limit Theorem}
    Again, we sample \jlinl{rand} as according to the task. We have
    have the following code.
    \begin{jllisting}[gobble=8]
        N = 10_000
        x = rand(Float64, (12, N))
        ms = mean(x, dims=1)
        
        step_size = 0.01
        edges = 0.0:step_size:1.0
        bins = histcounts(ms, edges)

        pdf = bins/sum(bins*step_size)
        sum(pdf*step_size)

        mₓ = mean(ms)
        σₓ = std(ms)^2

        @printf "mₓ: %.3f" mₓ  # -> mₓ: 0.499
        @printf "σₓ²: %.3f" σₓ # -> σₓ²: 0.007
    \end{jllisting}
\end{frame}
\begin{frame}[fragile]
    We plot the estimated PDF with the following code
    \begin{jllisting}[gobble=8]
        plot(edges[1:end-1], pdf, seriestype=:steppre)
        plot!(background_color=:transparent,foreground_color=:white)
        xlabel!("Value"); ylabel!("Propability")
        title!("PDF of CLT experiment")
        savefig("clt_pdf.svg")
    \end{jllisting}
    \pause
    \begin{figure}
        \includesvg[height=0.6\textheight]{"../clt_pdf.svg"}
    \end{figure}
\end{frame}


\begin{frame}
    \frametitle{Theoretical mean of CTF experiment [$m_z$]}
    We start by noting the random variable for our experiment can be defined as 
    $Z = \frac{1}{12}\sum_{i=1}^{12}X_i$.\\
    \pause
    \medskip 
    Then
    \begin{equation*}
        \E{Z} = \E{\frac{1}{12}\sum_{i=1}^{12}{X_i}} = \frac{1}{12}
        \E{\sum_{i=1}^{12}{X_i}}=\newline \frac{1}{12}\sum_{i=1}^{12}{\E{X_i}}
    \end{equation*} 
    \pause
    As we know that $\E{X_i}=\frac{1}{2}$, we have
    \begin{equation*}
        m_z = \E{Z} = \frac{1}{12}\sum^{12}_{i=1}{\frac{1}{2}}=\frac{1}{2}
    \end{equation*}
    % $\E{(X_i - \bar{X_i})^2} = \frac{1}{12}$
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We want to find the variance $\sigma_x^2 = \E{(Z-\bar{Z})^2}$, and using 
    the definition from the previous slide, we have
    \begin{equation*}
        \E{(Z-\E{Z})^2}=\E{\left(\frac{1}{12}\sum^{12}_{i=1}{X_i} -
        \E{\frac{1}{12}\sum^{12}_{i=1}{X_i}}\right)^2}
    \end{equation*}
    \pause
    We simplify, using
    \begin{equation}
        \E{X+Y} = \E{X} + \E{Y}
    \end{equation}
    and get the following
    \begin{equation*}
        \E{(Z-\E{Z})^2}=\E{\left(\frac{1}{12}\sum^{12}_{i=1}{X_i} 
                - \frac{1}{12}
        \sum^{12}_{i=1}{\E{X_i}}\right)^2}
    \end{equation*}
\end{frame}
\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We factorize and combine our sums to get
    \begin{equation*}
        \E{(Z-\E{Z})^2} = \left(\frac{1}{12}\right)^2
        \E{\left(\sum^{12}_{i=1}{X_i - \E{X_i}}\right)^2}
    \end{equation*}
    \pause
    Expanding the binomial, we get
    \begin{align*}
        = & \frac{1}{12^2}\sum^{12}_{i=1}{\E{\left(X_i - \E{X_i}\right)^2}} \\
        + & 2 \sum^{j\rightarrow12}_{\substack{i=1\\j=2\\i<j}}
        {\E{(X_i - \E{X_i})(X_j - \E{X_j})}}
    \end{align*}
    Which almost looks good, but there is an ugly term from the binomial
    expansion which ruins the fun.
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    There is luckely an easy clean-up for this. We know that our separate 
    observations $X_i$ are all independent. This means that each
    $\left(X_i-\E{X_i}\right) \text{ for } i=1,2,\dots,12$ are independent. \\
    \pause
    We can then use the multiplicative property
    \begin{alertblock}{Multiplicative Property}
        For two independent variables $X$ and $Y$, the expectation of their
        product is equal to the product of the expectation of each variable.
        \begin{equation*}
            \E{XY} = \E{X}\E{Y}
        \end{equation*}
    \end{alertblock}
    \pause
    This allows us to rewrite the additional binomial term as
    \begin{equation*}
        % \E{\left(X_i-\E{X_i}\right)\left(X_j-\E{X_j}\right)} = 
        \E{X_i-\E{X_i}}\cdot\E{X_j-\E{X_j}}
    \end{equation*}
\end{frame} 

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    We then utilize e.q.(3) on each factor of the binomial term,
    and we see that 
    \begin{gather*}
        \E{X_i-\E{X_i}} = \E{X_i}-\E{\E{X_i}}  =  0 \\
        \Downarrow \\
        2 \sum^{j\rightarrow12}_{\substack{i=1\\j=2\\i<j}}
        { \E{X_i-\E{X_i}}\E{X_j-\E{X_j}}} = 0
    \end{gather*}
    And thus the big bad evil term dissapered.
\end{frame}

\begin{frame}
    \frametitle{Theoretical variance of CTF experiment [$\sigma_z^2$]}
    Now that we know the additional term equates to 0 and we know
    $\E{\left(X_i - \E{X_i}\right)^2}=\frac{1}{12}$, we insert into our
    equation and get
    values into our equation
    \begin{align*}
        \E{\left(Z-\E{Z}\right)^2} 
        & = \frac{1}{12^2}\sum^{12}_{i=1}
        {\E{\left(X_i - \E{X_i}\right)^2}} \\
        \sigma_z^2  & = \frac{1}{12^2}\sum^{12}_{i=1}
        {\frac{1}{12}} = \frac{1}{144} \approx 0.0069
    \end{align*}
\end{frame}

\begin{frame}[fragile]
    \frametitle{Comparing the theoretical and estimated values}
    We got our theoretical values to be $m_z=0.5,\; \sigma_z^2=0.0069$.
    Comparing this to our result from earlier, we see that our estimated
    values of $m_z=0.499,\; \sigma_z^2=0.007$ is pretty much spot on. \\
    \medskip
    We then plot a gaussian curve over our previous plot with these values
    using the following code
    \begin{jllisting}[gobble=8]
        theoretical_pdf = (x) -> 1.0/√(2π*σₓ²)*exp(-(x-mₓ).^2 ./ (2σₓ²))
        plot(edges[1:end-1], pdf, seriestype=:steppost, label="Estimate")
        plot!(edges[1:end-1], theoretical_pdf.(edges[1:end-1]), label="Theoretical")
        plot!(background_color=:transparent,foreground_color=:white)
        xlabel!("Value"); ylabel!("Propability")
        title!("PDF of CLT experiment")
        savefig("clt_pdf_theoretical.svg")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{Plot of theoretical and estimated PDF}
    \begin{figure}
        \includesvg[width=0.9\columnwidth]{"../clt_pdf_theoretical.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2 - Stationarity and ergodicity}
    \pause
    We start by defining the functions as described in the task. The Julia
    version of the code looks pretty similar to that of MATLAB.
    \begin{jllisting}[gobble=8]
        function rp1(M,N)
            a = 0.02; b = 5;
            Mc = ones(M,1)*b*sin.((1:N)*π/N)'
            Ac = a*ones(M,1)*vec(1:N)';
            return (rand(M,N).-0.5).*Mc + Ac
        end
        function rp2(M,N)
            Ar = rand(M,1)*ones(1,N);
            Mr = rand(M,1)*ones(1,N); 
            return ( rand(M,N) .- 0.5 ).*Mr + Ar;
        end
        function rp3(M,N)
            a = 0.5; m = 3;
            return ( rand(M,N) .- 0.5 ) .* m .+ a;
        end
    \end{jllisting}
\end{frame}
 
\begin{frame}[fragile]
    \frametitle{2a - Stationarity or ergodic?}
    We then create the M=4 realizations of each process with N=100 points
    in time.
    \begin{jllisting}[gobble=8]
        M = 4; N = 100
        r1, r2, r3 = rp1(M,N), rp2(M,N), rp3(M,N)
        plot(
            [r1', r2', r3'], layout = (3,1), 
            title=["rp1" "rp2" "rp3"], legend=false,
            background_color=:transparent,
            foreground_color=:white
        )
        xlabel!("N"); ylabel!("Value")
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{2a - Stationarity or ergodic?}
    We get the following plot
    \begin{figure}
        \includesvg[width=\columnwidth]{"../rp1.svg"}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{2a - Stationarity or ergodic?}
    By inspection, we see that
    \begin{itemize}
        \item rp1 tends to increase in value $\rightarrow$ not stationary
        \item rp2 spreads out then narrows again $\rightarrow$
            not stationary
        \item rp3 stays pretty much the same $\rightarrow$ stationary
    \end{itemize}
    We then note that
    \begin{itemize}
        \item rp1 is and for early N, will not equal time average
            $\rightarrow$ not ergodic
        \item rp2 seems to have different time average on green and blue
            realization which would not match ensemble average $\rightarrow$
            not ergodic
        \item rp3 seems to be quite similar over time and realizations
            $\rightarrow$ seems ergodic
    \end{itemize}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2b - Ensemble averages}
    We compute the ensemble mean and std of each process
    and plot them with the following code
    \begin{jllisting}[gobble=8]
        M = 80; N = 100
        r1, r2, r3 = rp1(M,N), rp2(M,N), rp3(M,N)

        means = [mean(r1,dims=1); mean(r2,dims=1);mean(r3,dims=1)]
        stds =  [std(r1,dims=1);  std(r2,dims=1); std(r3,dims=1)]

        plot(
            [means', stds'], layout = (2,1),
            label=["rp1" "rp1" "rp2" "rp2" "rp3" "rp3"],
            background_color=:transparent,
            foreground_color=:white,
            title=["Means" "STDs"]
        )
    \end{jllisting}
\end{frame}

\begin{frame}
    \frametitle{2b - Ensemble averages}
    We then get the following plot
    \begin{figure}
        \includesvg[width=\columnwidth]{"../ensemble.svg"}
    \end{figure}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2c - Time averages}
    We estimate the time averages of each process by
    \begin{jllisting}[gobble=8]
        M = 4; N = 1000
        r1, r2, r3 = rp1(M,N), rp2(M,N), rp3(M,N)

        @printf "rp1 | Time averages: [%5.2f, %5.2f, %5.2f, %5.2f]" mean(r1,dims=2)...
        @printf "rp2 | Time averages: [%5.2f, %5.2f, %5.2f, %5.2f]" mean(r2,dims=2)...
        @printf "rp3 | Time averages: [%5.2f, %5.2f, %5.2f, %5.2f]" mean(r3,dims=2)...
    \end{jllisting}
\end{frame}

\begin{frame}[fragile]
    \frametitle{2c - Time averages}
    We get the following output
    \begin{jllisting}[gobble=8]
        # -> rp1 | Time averages: [10.09, 10.04, 10.00, 10.04]
        # -> rp2 | Time averages: [ 0.55,  0.11,  0.07,  0.95]
        # -> rp3 | Time averages: [ 0.55,  0.52,  0.57,  0.46]
    \end{jllisting}
    We gather from this that rp3 is ergodic, while rp1 and rp2 are not.
    \pause
    Displaying each process as images is pretty streight forward aswell with
    \begin{jllisting}[gobble=8]
        l = @layout [rp1 ; rp2 ; rp3]
        h1 = heatmap(r1)
        h2 = heatmap(r2)
        h3 = heatmap(r3)
        plot(
            h1, h2, h3, layout = l,
            title=["rp1" "rp2" "rp3"]
        )
    \end{jllisting}
\end{frame}

\begin{frame}
    This gives us the following images
    \begin{figure}
        \includesvg[width=0.7\columnwidth]{"../images.svg"}
    \end{figure}
    \pause
    Here we can clearly see that rp1 and rp2 are not ergodic as no individual
    random variable are able to represent the enitre domain of each process.
    rp3 however, satisfy this criteria, and match with our previous findings
    about its ergodicity.
\end{frame}

\end{document}
